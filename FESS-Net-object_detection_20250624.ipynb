{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56aaa8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "\n",
    "import torchvision \n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.cuda.amp import autocast, GradScaler \n",
    "\n",
    "import time \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18fb8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46bf17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa9b5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb7bb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection.engine import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e67611d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file_path = r'..'\n",
    "project_path=r'..'# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5855e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26d60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "pin_memory_bool=True#pin_memory\n",
    "use_amp = True#Automatic Mixed Precision\n",
    "#####optiizer_settings\n",
    "optimizer_ft = None\n",
    "weight_decay_setting = 1e-08\n",
    "eps_setting = 1e-08\n",
    "#-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad37563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "BNorm_decay_setting = 0.\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae319c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_detection_model_NViT_lite(num_classes, pretrained=True):\n",
    "    \n",
    "    from collections import OrderedDict\n",
    "    from detection.my_faster_rcnn import FasterRCNN\n",
    "    from torchvision.models.detection.rpn import AnchorGenerator\n",
    "    from detection.backbone_utils import BackboneWithFPN\n",
    "\n",
    "    from NextViT.nextvit_lite import nextvit_small\n",
    "\n",
    "    model_ft = nextvit_small()\n",
    "\n",
    "    ####################\n",
    "    layer_list = None\n",
    "    for layer in  model_ft.stem:\n",
    "        if layer_list is None:\n",
    "            layer_list = nn.Sequential(layer)\n",
    "        else:\n",
    "            layer_list.append(layer)\n",
    "\n",
    "    for layer in  model_ft.features:\n",
    "        if layer_list is None:\n",
    "            layer_list = nn.Sequential(layer)\n",
    "        else:\n",
    "            layer_list.append(layer)\n",
    "\n",
    "    layer_list.append(model_ft.norm)\n",
    "    ##########################3\n",
    "    backbone = layer_list\n",
    "    \n",
    "    ###############################\n",
    "    global return_layers\n",
    "    return_layers = {'6': '0', '10': '1', '20': '2', '23': '3',}\n",
    "    \n",
    "    in_channels_list = [96, 256, 512, 256, ]\n",
    "    \n",
    "    global FPN_out_channels,using_dropout\n",
    "\n",
    "    backbone = BackboneWithFPN(backbone,return_layers, in_channels_list,FPN_out_channels,dropout = using_dropout)\n",
    "    #####################\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "            featmap_names=['0','1','2','3'],\n",
    "            output_size=7,\n",
    "            sampling_ratio=2\n",
    "        )\n",
    "    ###########\n",
    "    global anchor_sizes\n",
    "    \n",
    "    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "    anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "    ##################################\n",
    "\n",
    "    model = FasterRCNN(\n",
    "            backbone=backbone,\n",
    "            num_classes=num_classes,\n",
    "            rpn_anchor_generator=anchor_generator,\n",
    "            box_roi_pool=roi_pooler,\n",
    "            min_size = sample_min_size, max_size = sample_max_size\n",
    "        ) \n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a64a600-25d5-4520-84d5-725c576aa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    from torchvision.transforms import v2\n",
    "\n",
    "    ###################################################\n",
    "    global my_transforms_train, sample_min_size,data_dir\n",
    "    '''my_transforms_train = v2.Compose(\n",
    "        [\n",
    "            v2.ToImage(),\n",
    "            ###\n",
    "            v2.Resize(size=(sample_min_size,sample_min_size),antialias=True),\n",
    "            \n",
    "            v2.RandomApply(torch.nn.ModuleList([v2.ColorJitter(0.5,0.5,0.5),]),p = Cjit_prob),\n",
    "            ###\n",
    "            v2.RandomHorizontalFlip(p = RHF_prob), \n",
    "            ####\n",
    "            v2.RandomVerticalFlip(p = RVF_prob),\n",
    "            ####\n",
    "            #v2.RandomApply(torch.nn.ModuleList([v2.RandomRotation(degrees=180,\n",
    "            #interpolation=transforms.InterpolationMode.BILINEAR)]), p=Rotation_prob),\n",
    "            ##\n",
    "            v2.RandomGrayscale(p = Rgray_prob),\n",
    "            ###\n",
    "            v2.RandomAutocontrast(p = Rcontrast_prob),\n",
    "            ###\n",
    "            v2.RandomApply(torch.nn.ModuleList([v2.GaussianBlur(kernel_size = 5),]),p = Gausblur_prob),\n",
    "            ###\n",
    "            v2.ToDtype(torch.float32, scale = False),\n",
    "        ]\n",
    "    )'''\n",
    "    ######################################################3\n",
    "    global my_transforms_val\n",
    "    my_transforms_val = v2.Compose(\n",
    "        [            \n",
    "            v2.ToImage(), \n",
    "            v2.Resize(size=(sample_min_size,sample_min_size),antialias=True),\n",
    "            v2.ToDtype(torch.float32, scale = False),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa471e2-abc2-4ed7-9918-c99cfeaf2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    get_transforms()\n",
    "    ######################\n",
    "    global image_datasets\n",
    "    image_datasets = {}\n",
    "    train_anno_json = f'train-annotation-{data_dir[-1]}.json'\n",
    "    #################################################################################\n",
    "    '''image_datasets['train'] = datasets.CocoDetection(root= os.path.join(data_dir, 'train'), \n",
    "                                annFile = os.path.join(data_dir, 'anno_dir',train_anno_json),\n",
    "                                                     transforms = my_transforms_train,                                                 \n",
    "                                        )'''\n",
    "\n",
    "    #########################################################################################################\n",
    "    \n",
    "    if 'DIOR20' in data_dir or 'MAR20' in data_dir or 'ShipRS50' in data_dir:\n",
    "        data_dir_val = os.path.dirname(data_dir)\n",
    "    else:\n",
    "        data_dir_val =data_dir\n",
    "        \n",
    "    image_datasets['val'] = datasets.CocoDetection(root = os.path.join(data_dir_val, 'val'), \n",
    "                                annFile = os.path.join(data_dir, 'anno_dir','val-annotation.json'), \\\n",
    "                                                       transforms = my_transforms_val, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "691271e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_datasets():\n",
    "    global image_datasets\n",
    "    '''image_datasets['train'] = datasets.wrap_dataset_for_transforms_v2(image_datasets['train'], \\\n",
    "            target_keys= [\"boxes\", 'labels','image_id'])'''\n",
    "    \n",
    "    image_datasets['val'] = datasets.wrap_dataset_for_transforms_v2(image_datasets['val'], \\\n",
    "                target_keys= [\"boxes\", \"labels\",'area','image_id','iscrowd'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728bda8",
   "metadata": {},
   "source": [
    "def get_datasets_fix_resize():\n",
    "    get_transforms()\n",
    "    ######################\n",
    "    global image_datasets_fix\n",
    "    image_datasets_fix = {}\n",
    "\n",
    "    #########################################################################################################\n",
    "    \n",
    "    if 'DIOR20' in data_dir or 'MAR20' in data_dir or 'ShipRS50' in data_dir:\n",
    "        data_dir_val = os.path.dirname(data_dir)\n",
    "    else:\n",
    "        data_dir_val =data_dir\n",
    "        \n",
    "    image_datasets_fix['val'] = datasets.CocoDetection(root = os.path.join(data_dir_val, 'val'), \n",
    "                                annFile = os.path.join(data_dir, 'anno_dir','val-annotation.json'), \\\n",
    "                                                       transforms = my_transforms_val, \n",
    "                                                  )\n",
    "    image_datasets_fix['val'] = datasets.wrap_dataset_for_transforms_v2(image_datasets_fix['val'], \\\n",
    "                target_keys= [\"boxes\", \"labels\",'area','image_id','iscrowd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c3ccf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders():\n",
    "    wrap_datasets()\n",
    "    ##############\n",
    "    global dataloaders,pin_memory_bool,persistent_workers\n",
    "    dataloaders ={}\n",
    "    '''dataloaders['train']= torch.utils.data.DataLoader(image_datasets['train'], batch_size = train_Bsize,\n",
    "            shuffle=True, pin_memory=pin_memory_bool, persistent_workers = persistent_workers,\\\n",
    "                                                      num_workers =loader_workers, \\\n",
    "                                                      collate_fn=lambda batch: tuple(zip(*batch)))'''\n",
    "    dataloaders['val'] =  torch.utils.data.DataLoader(image_datasets['val'], batch_size = val_Bsize,\\\n",
    "            shuffle=True, pin_memory=pin_memory_bool, persistent_workers = persistent_workers,\\\n",
    "                                                      num_workers=loader_workers,\\\n",
    "                                                      collate_fn=lambda batch: tuple(zip(*batch))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1daf0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_weight(module):\n",
    "    group_decay = []\n",
    "    group_no_decay = []\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "        elif isinstance(m, torch.nn.modules.conv._ConvNd):\n",
    "            group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "        elif isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "\n",
    "    assert len(list(module.parameters())) == len(group_decay) + len(group_no_decay)\n",
    "    groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay = BNorm_decay_setting)]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec441e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer():\n",
    "    \n",
    "    global model_ft,optimizer_ft,lr_init_RCNN\n",
    "    \n",
    "    params_grouped = group_weight(model_ft)\n",
    "\n",
    "    optimizer_ft = optim.AdamW(params_grouped,\\\n",
    "                               lr=lr_init_RCNN,betas=(0.9,0.999),eps = eps_setting,\\\n",
    "                               weight_decay = weight_decay_setting)\n",
    "    #############################################\n",
    "    global exp_lr_scheduler\n",
    "    exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer_ft, \n",
    "                                T_max=T_max_setting, eta_min=eta_min_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fabdec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model_ft():\n",
    "    global model_ft\n",
    "    global num_classes,sample_min_size,sample_max_size,loader_workers,train_Bsize,val_Bsize,image_input_size\n",
    "    if 'DIOR20' in data_dir:\n",
    "        num_classes = 21 \n",
    "        sample_min_size = image_input_size \n",
    "        sample_max_size = image_input_size\n",
    "        #val_Bsize = 24\n",
    "    elif 'NWPU-VHR10' in data_dir:\n",
    "        num_classes = 11 \n",
    "        sample_min_size = image_input_size \n",
    "        sample_max_size = image_input_size\n",
    "        #val_Bsize = 16\n",
    "    elif 'MAR20' in data_dir:\n",
    "        num_classes = 21 \n",
    "        sample_min_size = image_input_size \n",
    "        sample_max_size = image_input_size\n",
    "    elif 'ShipRS50' in data_dir:\n",
    "        num_classes = 51 \n",
    "        sample_min_size = image_input_size \n",
    "        sample_max_size = image_input_size\n",
    "    #\n",
    "    \n",
    "    if model_selection == 'Faster-RCNN-NViT-fpn-lite':\n",
    "        model_ft = my_get_detection_model_NViT_lite(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c796de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_for_training():\n",
    "    global val_in_coco,logfile_path\n",
    "    \n",
    "    get_model_ft()\n",
    "    \n",
    "    get_datasets()\n",
    "\n",
    "    get_dataloaders()\n",
    "    \n",
    "    \n",
    "    val_in_coco = convert_to_coco_api(dataloaders['val'].dataset)\n",
    "    \n",
    "    get_optimizer()\n",
    "\n",
    "    #get_recordfiles()\n",
    "\n",
    "    #get_setting_log()\n",
    "\n",
    "    #global logfile_path\n",
    "    \n",
    "    #logfile_path = os.path.join(project_path,training_logfile_name)\n",
    "\n",
    "    print('model is ready for training & testing!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28bb96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate_resize(model, data_loader, device):\n",
    "    from detection.engine import _get_iou_types\n",
    "    from detection.coco_eval import CocoEvaluator\n",
    "    from detection.utils import MetricLogger\n",
    "    \n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = \"Test:\"\n",
    "\n",
    "    global val_in_coco\n",
    "    coco = val_in_coco\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44d3780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_coco_api(ds):\n",
    "    from pycocotools.coco import COCO\n",
    "    \n",
    "    coco_ds = COCO()\n",
    "    # annotation IDs need to start at 1, not 0, see torchvision issue #1530\n",
    "    ann_id = 1\n",
    "    dataset = {\"images\": [], \"categories\": [], \"annotations\": []}\n",
    "    categories = set()\n",
    "    for img_idx in range(len(ds)):\n",
    "        # find better way to get target\n",
    "        # targets = ds.get_annotations(img_idx)\n",
    "        img, targets = ds[img_idx]       \n",
    "                \n",
    "        image_id = targets[\"image_id\"]\n",
    "        img_dict = {}\n",
    "        img_dict[\"id\"] = image_id\n",
    "        img_dict[\"height\"] = img.shape[-2]\n",
    "        img_dict[\"width\"] = img.shape[-1]\n",
    "        dataset[\"images\"].append(img_dict)\n",
    "        bboxes = targets[\"boxes\"].clone()\n",
    "        bboxes[:, 2:] -= bboxes[:, :2]\n",
    "        bboxes = bboxes.tolist()\n",
    "        labels = targets[\"labels\"].tolist()\n",
    "        areas = targets[\"area\"]#.tolist()\n",
    "        if 'iscrowd' in targets:\n",
    "            iscrowd = targets[\"iscrowd\"]\n",
    "        if \"masks\" in targets:\n",
    "            masks = targets[\"masks\"]\n",
    "            # make masks Fortran contiguous for coco_mask\n",
    "            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)\n",
    "        if \"keypoints\" in targets:\n",
    "            keypoints = targets[\"keypoints\"]\n",
    "            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()\n",
    "        num_objs = len(bboxes)\n",
    "        for i in range(num_objs):\n",
    "            ann = {}\n",
    "            ann[\"image_id\"] = image_id\n",
    "            ann[\"bbox\"] = bboxes[i]\n",
    "            ann[\"category_id\"] = labels[i]\n",
    "            categories.add(labels[i])\n",
    "            #ann[\"area\"] = areas[i]\n",
    "            ###fix height & width\n",
    "            box_height = bboxes[i][2]\n",
    "            box_width = bboxes[i][3]\n",
    "            ann[\"area\"] = box_height * box_width\n",
    "            ######            \n",
    "            ann[\"iscrowd\"] = iscrowd[i]\n",
    "            ann[\"id\"] = ann_id\n",
    "            if \"masks\" in targets:\n",
    "                ann[\"segmentation\"] = coco_mask.encode(masks[i].numpy())\n",
    "            if \"keypoints\" in targets:\n",
    "                ann[\"keypoints\"] = keypoints[i]\n",
    "                ann[\"num_keypoints\"] = sum(k != 0 for k in keypoints[i][2::3])\n",
    "            dataset[\"annotations\"].append(ann)\n",
    "            ann_id += 1\n",
    "    dataset[\"categories\"] = [{\"id\": i} for i in sorted(categories)]\n",
    "    coco_ds.dataset = dataset\n",
    "    coco_ds.createIndex()\n",
    "    return coco_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87bfcb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_workers = 2\n",
    "\n",
    "pin_memory_bool = False\n",
    "\n",
    "persistent_workers = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5e93b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_seed_value = np.random.randint(low=0, high=999)\n",
    "\n",
    "train_Bsize = 8\n",
    "\n",
    "accum_step = 4\n",
    "\n",
    "train_epochs = 60 * 4\n",
    "\n",
    "T_max_setting = train_epochs \n",
    "\n",
    "accum_step_lr_scheduler = train_epochs // T_max_setting\n",
    "\n",
    "lr_init_RCNN = 5e-5\n",
    "\n",
    "eta_min_setting = lr_init_RCNN * 0.2\n",
    "\n",
    "FPN_out_channels = 256\n",
    "\n",
    "val_Bsize = 32\n",
    "\n",
    "using_dropout = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bec1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_list = [r'/home/jason/data/DIOR20_dataset/25%-Train-ratio-A',\n",
    "                 \n",
    "                ]\n",
    "#--------------------------\n",
    "global data_dir\n",
    "data_dir = data_dir_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "379d8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection = 'Faster-RCNN-NViT-fpn-lite'\n",
    "\n",
    "Roi_align_layers = ['0','1','2','3',]\n",
    "\n",
    "anchor_sizes = ((24,), (48,), (96,), (144,), (192,))\n",
    "\n",
    "image_input_size = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70d15a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faster rcnn using custom SmoothL1Loss is activated!\n",
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "model is ready for training & testing!!!\n"
     ]
    }
   ],
   "source": [
    "init_for_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f9bbba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = torch.load(os.path.join('weights', \\\n",
    "                                        'DIOR20_TR25%A_20250204075831_Faster-RCNN-NViT-fpn-lite.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59fc3a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faster-RCNN-LHViT-fpn <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "print(f'Faster-RCNN-LHViT-fpn {model_ft.load_state_dict(model_weights)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "993fab5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [  0/367]  eta: 0:10:30  model_time: 0.8741 (0.8741)  evaluator_time: 0.2818 (0.2818)  time: 1.7181  data: 0.5379  max mem: 5792\n",
      "Test:  [100/367]  eta: 0:03:42  model_time: 0.6631 (0.6644)  evaluator_time: 0.0881 (0.1177)  time: 0.8400  data: 0.0255  max mem: 5792\n",
      "Test:  [200/367]  eta: 0:02:19  model_time: 0.6640 (0.6643)  evaluator_time: 0.1377 (0.1216)  time: 0.8511  data: 0.0252  max mem: 5792\n",
      "Test:  [300/367]  eta: 0:00:56  model_time: 0.6822 (0.6686)  evaluator_time: 0.1090 (0.1234)  time: 0.8470  data: 0.0258  max mem: 5792\n",
      "Test:  [366/367]  eta: 0:00:00  model_time: 0.6643 (0.6684)  evaluator_time: 0.0958 (0.1274)  time: 0.8527  data: 0.0253  max mem: 5792\n",
      "Test: Total time: 0:05:10 (0.8453 s / it)\n",
      "Averaged stats: model_time: 0.6643 (0.6684)  evaluator_time: 0.0958 (0.1274)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=2.46s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.538\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.807\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.589\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.243\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.532\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.708\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.324\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.555\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.610\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.333\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.624\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<detection.coco_eval.CocoEvaluator at 0x7f5cd8654220>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_resize(model_ft, dataloaders['val'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3824d23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0+cu118'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0cdc4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.0+cu118'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00685af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 10 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "758db54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
